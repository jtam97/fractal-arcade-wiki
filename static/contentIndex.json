{"Games/MarvelRivals":{"title":"Marvel Rivals - In-depth look at portal mechanics","links":[],"tags":[],"content":"Marvel Rivals is a free-to-play team shooter, a Marvel version of Overwatch and one of my current favorite competitive games."},"Games/PathOfExile":{"title":"Path of Exile - Same Map, Different Experience","links":[],"tags":[],"content":"I‚Äôve been playing Path of Exile for years now - countless leagues and countless more characters. I‚Äôll give a basic rundown of the game, if you‚Äôre familiar you can skip to the algorithm section.\nGameplay\nPath of Exile is a loot-based action RPG with an insanely complex build and item system that makes each character a project in itself. The main thing I‚Äôm going to focus on here is the unique map procedural generation. From my perspective as a player, the campaign is pretty cool but the maps specifically are what make the game special.\nCampaign Maps\nThe level design of campaign maps and endgame maps are interesting and unique, and on repeat playthroughs, you can see the level design elements change slightly but are still familiar. In a way, this type of map caters to casual players and more hardcore ‚Äúspeed runners‚Äù - maps are never identical so it doesn‚Äôt feel stale, but the pathing is familiar enough that, once you‚Äôve learned the map layout, you can move in a general direction and be confident you are going the right way.\nAlgorithm"},"Games/Portals":{"title":"Portal (2007) - A Spatial Illusion","links":["Games/MarvelRivals"],"tags":[],"content":"Portal, released by Valve in 2007, is a critically acclaimed puzzle game that‚Äôs a must-play for any puzzle game enthusiast.\nCore Mechanics\nPortal is a first-person puzzle game where the player must navigate through a series of puzzles using a portal gun. The portal gun can create two linked portals, an orange one and a blue one - when you enter one, you exit through the other. Two key mechanics make this particularly interesting:\n\nVisual Recursion - You can see through portals and into the other side. What makes this mind-bending is that you can see through portals recursively:\n\n\n\nMomentum Conservation - Objects maintain their velocity when passing through portals. This enables fascinating mechanics like the infinite fall:\n\n\nImplementation Approaches\nThis is an exercise I like to do when I‚Äôm learning about a new algorithm. I try to reason\nabout how I would implement it, and it really helps me understand the algorithm. Let‚Äôs explore how we might implement such a system. For clarity, let‚Äôs call the entrance portal ‚ÄúA‚Äù and the exit portal ‚ÄúB‚Äù.\nApproach 1: World Duplication\nThis idea was an initial knee jerk naive approach. One way that is probably overkill is to render the entire world twice - once on your side of portal A and another world for where portal B is. This is pretty lazy, but simple to code -\n1. Duplicate all assets in the world while maintaining positions of portal A and B\n2. Overlap portal A and portal B to connect the two worlds\n\nProblem: Extremely inefficient - you‚Äôre rendering the entire world twice and many assets are duplicated for no perspective benefit.\nApproach 2: Camera Centered at Other Portal\nAnother way that seems way better on hindsight is to set a camera centered at the other portal.\n1. Set a camera centered at the other portal\n2. Limit rendering the scene coming from the other camera to the portal area\n\nProblem: This is much more efficient without duplicating assets than Approach 1, but now you have the issue of adjusting the camera view through the other portal. If you don‚Äôt adjust the angle of the camera, it would just be one still picture which doesn‚Äôt look right.\nApproach for Portal Physics\nFor either of the above, how do we conserve momentum when going through the portal? I think it‚Äôs as simple as measuring the speed and angle in and duplicating the speed and angle out. The actual extra physics calculations (like gravity) can be done as soon as you are fully out of the portal without affecting trajectory much.\n1. Measure the speed and angle of entry at portal A\n2. Apply same speed and angle when exiting portal B\n3. Resume normal physics calculations after exit\n\nThe Solution: Stencil Buffer Rendering\nSo how do you render something like this graphically? The method they use is something called ‚Äústencil buffer portal rendering‚Äù. It is very similar to Approach 2, but adds a few steps to the rendering process. It is a concept very similar to looking through a mirror.\n1. Start with portal A and portal B.\n2. When looking at a portal, calculate the angle at which your view intersects the portal plane of portal A.\n3. Set the camera at the same distance and angle from portal B as from portal A. In the below diagram, you can see the red camera reflecting the same distance as the player.\n4. Limit rendering to portal B&#039;s visible area using stencil buffer.\n\n\nAn elegant and mathematically simple solution.\nModern Implementation\nWe‚Äôve been talking about the game Portal, but what about the newest rendition of portals in Marvel Rivals? Here‚Äôs a quick look at the portal shenanigans in Marvel Rivals.\n\nFor a deeper dive into Marvel Rivals‚Äô portal mechanics, check out my post on the topic."},"Science/AI-LLMs":{"title":"AI & LLMs","links":[],"tags":[],"content":"\n\n                  \n                  Information here was collected and interpreted from a lecture from Stanford CS229 - video: www.youtube.com/watch I highly highly recommend watching it, it is a fantastic lecture.\n                  \n                \n\nSummary\nThe big picture: LLMs are basically probability distributions for sequences of words. You train them on massive amounts of internet data (pretraining), then finetune them to be helpful assistants (post-training). The key insight: bigger models + more data = better performance, almost always. Architecture details matter less than you‚Äôd think.\nPretraining: Teach the model about language by having it predict the next word in sentences. It learns patterns from the entire internet. Think of it like reading everything and learning what words tend to come after other words.\nHowever, it‚Äôs a bit stupid in that if you give it a question like ‚ÄúWhat is the distance to the moon?‚Äù, it won‚Äôt answer but might give you the most related sentence after that which is ‚ÄúWhat is the distance to the sun?‚Äú.\nPost-training: The pretrained model gives you text related to your question, not necessarily what you asked for. Post-training (SFT + RLHF) teaches it to actually answer your questions in a helpful way. SFT shows it examples of good answers, RLHF trains it to generate answers humans prefer.\nFor example, you want it to always answer the question ‚ÄúWhat is the distance to the moon?‚Äù with ‚ÄúIt is 238,900 miles away.‚Äù It learns this format and when asked ‚ÄúWhat is the distance to the sun?‚Äù, it responds with ‚ÄúIt is ‚Ä¶ distance away.‚Äù Note that if the information doesn‚Äôt exist (like if we don‚Äôt know the distance to the sun), the LLM might fill in the blank with an incorrect answer.\nWhat actually matters: In practice, data quality(post-training ie. fine-tuning) and quantity(foundation model size ie. pre-training) matter way more than fancy architectures. Also evaluation is hard because answers are unbounded - you can‚Äôt just use standard metrics. And everyone‚Äôs bottlenecked by compute, so GPU optimization is crucial.\nThe bitter lesson: In the long run, only compute matters. More data + more compute, done well ‚Üí model gets better. Architecture tweaks are nice but scale is what really moves the needle.\n\nNotes:\nWhat matters when training LLMs?\nMore important in academia:\n\nArchitecture\nTraining algorithm/loss\n\nMore important in industry (what matters in practice):\n\nData\nEvaluation\nSystems\n\nPretraining vs post-training\n\nPretraining is getting the model to learn about the data - for example, learning the entire internet Large language models\nPost-training Post-training of LLM  is taking these models and turning them into AI assistants\n\nLarge language models\n\nEssentially learned probability distributions for sequences of words/tokens\nLLMs are generative - you can sample from the distribution to generate sentences\nAutoregressive (AR) language models:\n\nUse probability from all previous words in the sentence to predict the next word\nEssentially a for loop: generate a word, then loop back to generate the next\n\n\nLoss function: based on correctly guessing the next word in the sentence\n\nTokenizers\n\nExtremely important for LLMs\nMore generalizable than just words:\n\nHow do you match a word when there‚Äôs a typo?\nLanguages with no spaces can‚Äôt be tokenized just by space\n\n\nTokenizers use subsequences (~3-4 letters)\nExample: Byte pair encoding (BPE), one of the most common tokenizers\n\nTraining process:\n\nStart with a large corpus of text\nStart with one token per character\nMerge common pairs of tokens into one token\n\n\n\n\nPre-tokenization (for optimization):\n\nIf you consider every character as a token, it takes too long\nFor English: ensure tokens separated by spaces aren‚Äôt considered together\n\n\nKeep smaller tokens even with longer ones (e.g., for typos - represent typos by characters)\n\nLLM Evaluation\n\nPerplexity: 2^(average loss per token)\n\nLimited between 1 and vocabulary length\nIntuition: number of tokens you‚Äôre hesitating between\nNot really used academically anymore (not comparable - depends on tokenizer, data, etc.)\nStill very important in development\n\n\nAcademic evaluation: standard NLP benchmarks\n\nHELM from Stanford\nHugging Face LLM leaderboard\n\n\nCommon benchmark: MMLU (questions from many different subjects)\n\nData - Training on ‚Äúall of the internet‚Äù\n\nMany difficult steps to get clean data from the internet\nInteresting concepts from the process:\n\nModel-based filtering: use another classifier to predict if a page is from Wikipedia, then weigh it more in training\nDomain weighting: e.g., code data helps with general logic reasoning, so upweight it\nLearning rate annealing on high quality data: overfit model to high quality data at the end\n\n\n\nScaling Laws - More data and larger models = better performance\n\nNo such thing as overfitting with larger models - larger = better\nWhy scaling laws matter:\n\nYou can assume this is true and find ‚Äúscaling recipes‚Äù - formulas that tell you how to change hyperparameters\nUse same hyperparameters on smaller models and extrapolate to bigger models\nExample: Don‚Äôt know if LSTM or transformer is better?\n\nTry both at lower scale and observe ‚Äúscaling rate‚Äù (slope of scaling law)\n\n\nThings scaling laws help decide:\n\nTrain models longer vs train bigger models\nCollect more data vs get more GPUs\nData repetition/multiple epochs\nData mixture weighting\nArchitecture: LSTMs vs transformers?\nSize: width vs depth?\n\n\n\n\nThe bitter lesson: models improve with scale and Moore‚Äôs law\n\nIn the long run, only compute matters - architecture details don‚Äôt matter as much\nMore data + more compute, done well ‚Üí model naturally gets better\n\n\n\n\nTraining a SOTA model\n\nExample of current SOTA: LLaMa 3 400B\n\nData: 15.6T tokens\nParameters: 405B\n~40 tokens/param\n\n\nTotal FLOPS: 3.8 e25 FLOPs\nCompute: 16,000 H100 with avg throughput of 400 teraFLOPS\nTime: 26 million gpu hours = 70 days. From paper = ~30M gpu hours\nCarbon emitted: ~4400 ton CO2 equivalent or 2,000 round trip flights between JFK and Heathrow airport\nNext model: new models ~10x more FLOPs than previous model\n\n\nPost-training of LLM\n\nTurns a language model into an AI assistant\nWithout post-training: LLM gives text related to your input, not what you asked for\n\nExample input: ‚ÄúExplain the moon landing to a 6 year old‚Äù\nOutput: ‚ÄúExplain the theory of gravity to a 6 year old‚Äù + ‚ÄúExplain the theory of relativity to a 6 year old‚Äù\n\n\nProblem: desired behavior data is what we want but scarce and expensive\nPretraining data is not what we want but there‚Äôs a lot of it\nSolution: finetune pretrained LLM on a little bit of desired data ‚áí ‚Äúpost‚Äù-training\n\nPost-training Methods\n\nSupervised finetuning (SFT)\n\nIdea: finetune LLM with language modeling of desired answers\n\ni.e., supervised next word prediction\nSame algorithm and loss function as pretraining, just different dataset\n\nDifferent hyperparameters from pretraining ‚Üí increased learning rate\n\n\n\n\nCollecting desired data: ask humans\n\nOpenAssistant (Kopf+, 2023) as data ‚Üí this finetuning turned GPT-3 into ChatGPT\n\n\nProblem: human data is slow to collect and expensive\nSolution: use LLMs to scale data collection ‚Üí have LLM generate questions and answers (e.g., 52,000 Q&amp;A pairs for SFT)\nHow much data for SFT?\n\nVery little needed! Only a few thousand examples\n\nDifference between 2k and 32k makes no difference\n\n\nMain thing it learns: format of desired answers (length, bullet points, etc.)\n\nAssumption: knowledge already in pretrained LLM, you‚Äôre just specializing response format\nExample: humans use bullet points for todo lists. Pretrained model has many ways to suggest a todo list, but SFT optimizes toward bullet points\n\n\n\n\n\nReinforcement Learning with Human Feedback (RLHF)\nProblem with SFT: behavior cloning of humans\n\nBound by human abilities: humans may prefer things they can‚Äôt generate\nHallucinations: cloning correct answers teaches LLM to hallucinate even if it didn‚Äôt know about it\n\nExample: if answer has a reference LLM never saw, it will make up a plausible-sounding reference\n\n\nCollecting ideal answers is expensive\n\nRLHF\n\nIdea: maximize human preference rather than cloning behavior\nPipeline:\n\nFor each instruction: generate 2 answers from a pretty good model (SFT)\nAsk labelers to select their preferred answers\nFinetune LLM to generate more preferred answers\n\n\n\nHow do we tune the model to generate preferred answers?\nReinforcement learning using proximal policy optimization (PPO) algorithm\n\nWhat is the reward?\n\nOption 1: whether model is preferred to baseline?\n\nIssue: binary reward lacks information about why it‚Äôs preferred\n\n\nOption 2: train a reward model R using logistic regression loss to classify preferences\n\nConverts to continuous measure: assigns number for how much humans prefer an answer\n\n\n\n\nNote: LM after PPO is no longer modeling distribution - it represents your policy from RL\n\nModel may give only single answer generation - not every possible thing humans might say (not a distribution)\n\n\nIn theory: RL is great, and this is exactly what ChatGPT did (used PPO to generate preferred answers)\n\nProblem: RL is extremely messy to implement in practice\n\n\n\nRLHF using Direct Preference Optimization (DPO)\n\nEssentially a simplification of PPO\nIdea: maximize probability of preferred output, minimize others\n\nSimpler: no additional RL model after calculating maximum likelihood\nSimilar global minima, so functionally works as well as RL\n\n\nNote: DPO cannot use unlabeled data to improve its model\n\nChallenges with RLHF: Human data\n\nSlow and expensive to collect\nHard to focus on correctness rather than form (e.g., length)\nAnnotator distribution shifts:\n\nAnnotators from different backgrounds prefer different things\n\n\nCrowdsourcing ethics\n\nRLHF using LLM Data\n\nIdea: replace human preferences with LLM preferences\nPrice to human agreement ratio: \n\nHumans only agree with themselves 66% of the time\nLLM-generated examples: 63% agreement\n\n\nThis has become standard in open source community\n\nEvaluation of Post-training\n\nHow do we evaluate ChatGPT? Answers are unbounded ‚Üí can be any answer in the whole database\nChallenges:\n\nCan‚Äôt use validation loss to compare different methods\nCan‚Äôt use perplexity ‚Üí not calibrated, and some aligned LLMs are policies which can have extremely low perplexity\nLarge diversity of use cases:\n\nGeneration, Open QA, Brainstorming, Chat, Summarization\n\n\nOpen-ended tasks ‚Üí hard to automate evaluation\n\n\nSolution: ask for annotator preference between answers\n\nNow evaluating different answers from different models\n\n\n\nHuman evaluation: e.g., ChatBot Arena\n\nIdea: users interact (blinded) with two chatbots, rate which is better\n\nLLM Evaluation: e.g., AlpacaEval\n\nIdea: use LLM instead of humans\nSteps:\n\nFor each instruction: generate output by baseline and model to evaluate\nAsk GPT-4 which answer is better\n\n\nBenefits:\n\n98% correlation with ChatBot Arena\nCheap and fast: &lt;3 minutes and &lt;$10 to run\n\n\nChallenge: spurious evaluation\n\nExample: LLMs prefer longer outputs\nWays to control for this, but LLM bias can still propagate\n\n\n\nLLM Systems Design: GPUs\n\nProblem: everyone is bottlenecked by compute\nWhy not buy more GPUs?\n\nExpensive and scarce\nPhysical limitations (communication between GPUs)\n\n\nOne way to improve: massively parallelize\n\nSame instruction applied on all threads but with different inputs\nOptimize for throughput processing\n\n\nFast matrix multiplication\nCompute &gt; memory and communication\n\nGPUs process faster than they can be fed data\n\n\nMemory hierarchy:\n\nMemory closer to core ‚Üí faster but less memory\nMemory further from core ‚Üí slower but more memory\n\n\nMetric: model flop utilization (MFU)\n\nRatio: observed throughput / theoretical throughput\n50% is great\n\n\n\nOne way to solve: Low precision\n\nLower precision floats, fewer bits ‚Üí faster communication and lower memory consumption\nFor deep learning: remove decimal points in matrix multiplication ‚Üí 16 bits vs 32 bits\nFor training: automatic mixed precision (AMP)\n\nAnother way: Operator fusion\n\nProblem: each PyTorch line moves variable to global memory - multiple lines = multiple movements\nIdea: communicate once - fused kernel\n\nUse torch.compile() - rewrites PyTorch code into one operation\n\n\n\nOther optimizations:\n\nTiling and FlashAttention\nParallelization\nData parallelism: copy model and optimizer to each GPU\nModel parallelism via pipeline parallelization: each GPU has a different layer\n"},"Science/Laplace-Transform":{"title":"Laplace Transform","links":[],"tags":[],"content":"\n\n                  \n                  Information from 3Blue1Brown video series on Laplace Transform \n                  \n                \n\nNotes:\nBasics of Laplace Transform\nStarting off with exponential functions\n\nNote that the exponential function e^t is hugely important in explaining Euler‚Äôs formula. First the derivative of e^t is e\n\nIntuition using position and velocity vectors over time\n\nd/dt e^t = e^t means velocity is always equal to position\nd/dt e^2t = 2e^2t means velocity is always twice the position\nd/dt e^-0.5t = -0.5t e^-0.5t. The velocity vector is backwards and half of the position vector\nHere is the interesting part - d/dt e^it (multiplying by an imaginary number) = i e^it\n\nGeometrically, multiplying i to a vector is like making a 90 degree rotation\nThis means e^it traces a circle as the velocity vector is always perpendicular to the position vector (famously, e^i pi = -1)\n\n\n"},"Science/LinearAlgebra":{"title":"Linear Algebra","links":[],"tags":[],"content":"\n\n                  \n                  Information from 3Blue1Brown video series on Linear Algebra \n                  \n                \n\nThe basis of a coordinate system is the 1 unit vectors needed to map out every point in that space - i.e 1 unit for x and y maps out 2D system.\nSpan is all the points a combination of vectors can reach if you had any scalar value.\nLinear combination ‚Üí Linear independence vs dependence\n- Linearly dependent means vectors that do not expand the span\nLinear Transformations\nLinear transformations means gridlines must be evenly spaced and parallel to be linear\nA linear transformation of a vector can be described by the basis vectors using ai + bj. Interestingly, if the basis gets transformed, you can use the new coordinates of i and j to calculate the linear transformation of a vector."},"index":{"title":"Homepage","links":["Games/PathOfExile","Games/Portals","Games/MarvelRivals","Science/AI-LLMs","Science/LinearAlgebra","Science/Laplace-Transform"],"tags":[],"content":"\n\n                  \n                  Welcome to the Fractal Arcade Wiki! \n                  \n                \n\nThis is an exploration of unique algorithms and technical designs in games - sometimes beautiful, sometimes hacky, sometimes lazy - all of which make them a herculean feat of creativity and enjoyment.\n\n\n\n\n\n\nThe Fractal Arcade\n\nAn algorithm must be seen to be believed.\n‚Äî Donald Knuth\n\nThe name ‚ÄúFractal Arcade‚Äù, at its root, was inspired by fractals and chaos theory, often associated with infinite procedural generation. Just like fractals, the controlled chaos and beauty in games can be found in the details.\nIn this blog, I explore:\n\nüß† Computer science innovations\nüéØ Game mechanics\n‚ú® The brilliant combinations that make gaming so interesting\n\nI‚Äôll also have some posts here that are pure scientific conceptualization and intuition for my own learning!\n\nüëã A bit about me!\n\nI‚Äôm Justin Tam, a CS/bioinformatics researcher and PhD student, and a video game nerd! I love games but know little about what makes them tick. So this is my exploration of unique algorithms, innovative ideas and technical designs in games.\n\nMy top 5 games of all time:\n\nDark Souls 1 (and by extension, most of the souls-like games but if I included them, my top 5 would all be souls games)\nModern Warfare 2019\nLeague of Legends (aram/arena only player nowadays)\nValorant\nClair Obscur: Expedition 33\n\nüéÆ Current obsessions:\n\nBattlefield 6\nNightreign\n\n\nüìö Featured Articles\n\n\n                  \n                  Explore these deep dives into game design and mechanics \n                  \n                \n\n\nüó∫Ô∏è Path of Exile - Same Map, Different Experience In Progress\nüåÄ Portal (2007) - A Spatial Illusion\nü¶∏ Marvel Rivals - Or Portal 2.5 In Progress\n\n\n\n\n\n                  \n                  Explore some science conceptualization notes I took for my own understanding - it contains my intuition if I were to teach it \n                  \n                \n\n\nüß† AI &amp; LLMs - Notes from Stanford CS229 on large language models\nüìê Linear Algebra - Intuition from 3Blue1Brown‚Äôs series on linear transformations\nüîÑ Laplace Transform - Conceptual understanding from 3Blue1Brown\n\n\n"}}